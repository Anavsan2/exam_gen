import streamlit as st
import os
import json
import tempfile
import pytesseract
from pathlib import Path
from pdf2image import convert_from_path
from PIL import Image

# --- IMPORTACIONES CORREGIDAS ---
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
# ‚úÖ ESTA ES LA L√çNEA QUE DABA ERROR (SOLUCIONADA):
from langchain_core.documents import Document 

# --- CONFIGURACI√ìN DE LA P√ÅGINA ---
st.set_page_config(
    page_title="Generador de Ex√°menes con IA",
    page_icon="üìù",
    layout="centered"
)

st.title("üìù Generador de Ex√°menes a Medida")
st.markdown("""
Sube tus apuntes o libros (PDF) y elige cu√°ntas preguntas quieres para practicar.
""")

# --- FUNCIONES AUXILIARES (OCR y Limpieza) ---

def clean_json_string(json_str):
    """Limpia la respuesta del LLM para asegurar que es un JSON v√°lido."""
    json_str = json_str.strip()
    if "```json" in json_str:
        json_str = json_str.split("```json")[1]
    if "```" in json_str:
        json_str = json_str.split("```")[0]
    return json_str

def perform_ocr(pdf_path):
    """Realiza OCR en un PDF escaneado."""
    st.info("‚ö†Ô∏è Detectando im√°genes... Aplicando OCR (esto puede tardar unos segundos).")
    try:
        images = convert_from_path(pdf_path)
        text = ""
        progress_bar = st.progress(0)
        
        for i, image in enumerate(images):
            text += pytesseract.image_to_string(image) + "\n"
            progress_bar.progress((i + 1) / len(images))
        
        progress_bar.empty()
        return text
    except Exception as e:
        st.error(f"Error en OCR: {e}. Aseg√∫rate de tener instalado 'poppler-utils' y 'tesseract-ocr' en packages.txt.")
        return ""

def process_document(uploaded_file):
    """Procesa el PDF, extrae texto (normal u OCR) y crea el VectorStore."""
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_path = tmp_file.name

    try:
        loader = PyMuPDFLoader(tmp_path)
        docs = loader.load()
        text_content = "\n".join([doc.page_content for doc in docs])

        # Detectar si es escaneado (poco texto)
        if len(text_content.strip()) < 50:
            extracted_text = perform_ocr(tmp_path)
            if not extracted_text.strip():
                return None, "No se pudo extraer texto ni con OCR."
            docs = [Document(page_content=extracted_text, metadata={"source": uploaded_file.name})]

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        chunks = text_splitter.split_documents(docs)

        if not chunks:
            return None, "El documento est√° vac√≠o despu√©s del procesamiento."

        embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-large-en-v1.5")
        vectorstore = FAISS.from_documents(chunks, embeddings)
        
        return vectorstore, None

    except Exception as e:
        return None, str(e)
    finally:
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

# --- BARRA LATERAL (CONFIGURACI√ìN) ---
with st.sidebar:
    st.header("‚öôÔ∏è Configuraci√≥n")
    api_key = st.text_input("Groq API Key", type="password")
    
    st.divider()
    
    uploaded_file = st.file_uploader("Sube tu material (PDF)", type="pdf")
    
    st.subheader("Par√°metros del Examen")
    
    # Input num√©rico para elegir la cantidad exacta
    num_questions = st.number_input(
        "N√∫mero de preguntas:", 
        min_value=1, 
        max_value=20, 
        value=5, 
        step=1,
        help="Elige cu√°ntas preguntas quieres generar (m√°x 20 por bloque)."
    )
    
    level = st.selectbox("Nivel de dificultad", ["F√°cil", "Intermedio", "Dif√≠cil"])
    
    generate_btn = st.button("Generar Examen", type="primary")

# --- PROMPT REFORZADO ---
QUIZ_SYSTEM_PROMPT = """
You are an expert teacher creating a multiple-choice quiz based ONLY on the provided context.

STRICT REQUIREMENTS:
1. Difficulty Level: {level}
2. **Quantity: Generate EXACTLY {num_questions} questions.** Do not generate fewer or more.
3. Language: Spanish (Respond in Spanish).

OUTPUT FORMAT (JSON ONLY):
Return a raw JSON list of objects. No markdown, no explanations outside the JSON.
Schema:
[
   {{
       "question": "Texto de la pregunta",
       "options": ["Opci√≥n A", "Opci√≥n B", "Opci√≥n C", "Opci√≥n D"],
       "answer": 0,  // √çndice de la respuesta correcta (0-3)
       "explanation": "Breve explicaci√≥n de por qu√© es correcta seg√∫n el texto."
   }}
]
"""

# --- L√ìGICA PRINCIPAL ---

if generate_btn:
    if not api_key:
        st.warning("‚ö†Ô∏è Por favor, introduce tu API Key de Groq.")
    elif not uploaded_file:
        st.warning("‚ö†Ô∏è Por favor, sube un archivo PDF.")
    else:
        os.environ["GROQ_API_KEY"] = api_key
        
        # 1. Procesar documento (solo si es nuevo)
        if "vectorstore" not in st.session_state or st.session_state.get("current_file") != uploaded_file.name:
            with st.spinner("üîç Procesando documento..."):
                vectorstore, error = process_document(uploaded_file)
                if error:
                    st.error(f"Error: {error}")
                else:
                    st.session_state.vectorstore = vectorstore
                    st.session_state.current_file = uploaded_file.name

        # 2. Generar Preguntas
        if "vectorstore" in st.session_state:
            with st.spinner(f"ü§ñ Generando {num_questions} preguntas de nivel {level}..."):
                try:
                    # Recuperamos m√°s contexto si se piden muchas preguntas
                    # k = n√∫mero de preguntas * 1.5 (para asegurar suficiente info)
                    k_retrieval = max(5, int(num_questions * 1.5))
                    retriever = st.session_state.vectorstore.as_retriever(search_kwargs={"k": k_retrieval})
                    
                    # Recuperar contexto
                    docs = retriever.invoke("conceptos clave definiciones importantes resumen del tema")
                    context_text = "\n\n".join([d.page_content for d in docs])

                    # Llamar al LLM
                    llm = ChatGroq(model="llama-3.3-70b-versatile", temperature=0.5)
                    
                    prompt = ChatPromptTemplate.from_messages([
                        ("system", QUIZ_SYSTEM_PROMPT),
                        ("human", "Contexto: {context}")
                    ])
                    
                    chain = prompt | llm
                    response = chain.invoke({
                        "context": context_text,
                        "level": level,
                        "num_questions": num_questions
                    })
                    
                    # Procesar JSON
                    json_content = clean_json_string(response.content)
                    quiz_data = json.loads(json_content)
                    
                    # Validar cantidad (informativo)
                    if len(quiz_data) != num_questions:
                        st.warning(f"Nota: Se solicitaron {num_questions} preguntas, pero la IA gener√≥ {len(quiz_data)}.")
                    
                    # Guardar estado
                    st.session_state.quiz_data = quiz_data
                    # Reiniciar respuestas del usuario
                    st.session_state.user_answers = {} 
                    st.session_state.quiz_submitted = False
                    
                except Exception as e:
                    st.error(f"Error generando el examen: {e}")

# --- MOSTRAR EXAMEN ---
if "quiz_data" in st.session_state and st.session_state.quiz_data:
    st.write("---")
    st.subheader(f"üìù Examen ({len(st.session_state.quiz_data)} preguntas)")
    
    with st.form("quiz_form"):
        for i, q in enumerate(st.session_state.quiz_data):
            st.markdown(f"#### {i+1}. {q['question']}")
            
            # Recuperar respuesta previa si existe
            current_choice = st.session_state.user_answers.get(i, None)
            
            choice = st.radio(
                "Elige una opci√≥n:", 
                q['options'], 
                key=f"radio_{i}", 
                index=None if current_choice is None else q['options'].index(current_choice) if current_choice in q['options'] else None,
                label_visibility="collapsed"
            )
            st.write("") 
            
        submit = st.form_submit_button("Corregir Examen")
        
        if submit:
            st.session_state.quiz_submitted = True
            # Guardar las respuestas actuales del form en el estado
            for i in range(len(st.session_state.quiz_data)):
                st.session_state.user_answers[i] = st.session_state[f"radio_{i}"]

    # --- RESULTADOS ---
    if st.session_state.get("quiz_submitted", False):
        st.write("---")
        st.subheader("Resultados")
        
        correct_count = 0
        for i, q in enumerate(st.session_state.quiz_data):
            user_choice = st.session_state.user_answers.get(i)
            correct_option = q['options'][q['answer']]
            
            with st.expander(f"Pregunta {i+1}: {'‚úÖ Correcta' if user_choice == correct_option else '‚ùå Incorrecta'}", expanded=True):
                if user_choice == correct_option:
                    correct_count += 1
                    st.success(f"¬°Bien hecho! La respuesta es: {correct_option}")
                else:
                    st.error(f"Tu respuesta: {user_choice}")
                    st.success(f"Respuesta correcta: {correct_option}")
                
                st.info(f"üí° Explicaci√≥n: {q['explanation']}")

        if len(st.session_state.quiz_data) > 0:
            score = (correct_count / len(st.session_state.quiz_data)) * 100
            st.metric(label="Calificaci√≥n Final", value=f"{score:.0f}/100")
            
            if score == 100:
                st.balloons()
